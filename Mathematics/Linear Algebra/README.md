# Linear Algebra

#### Scalars
Scalars are just values that represent something. Just normal values.


#### Vectors

Operations:
-> Addition (Dot Product): Total work achieved through both vectors in quantified form

-> Scalar Multiplication: leads to vector growth or shrinks.

-> Projection: Shadow of a vector placed on other vector, helps in anaylizing features of unknown vectors.

#### Matrics

Composition of numbers, symbols or expressions in rectangular array, used to convert expressions into arrays

Operations:

-> Addition: Simply adding corresponding elements of 2 matrics, both matrics needs to be of same order

-> Mutliplication: multiplying the rows of matrix 1 with the columns of matrix 2,  the rows of matrix 1 need to be equal to column of matrix 2 for it to work.

-> Tranpose: interchanging rows and columnds of a matrix, used to flip the dimensions

-> Determinant: scalar value of the matrix, gives you the product of eigen values of the matrix

-> Inverse of the matrix: when multiplied with original gives identity matrix, helps in transformations efficently, some matrices dont have inverse means information is not reliable 

### Vectors as matrix

-> Vectors can be easily converted to matrix.

-> They make it easy to apply operations on the data. 

-> Operations: Scaling(increasing the size), rotation and shearing(reshaping).

-> Helps in solving the equation and obtain solutions.

### Eigen Vectors

-> They dont change even if the transformation is applied to them

-> sensitive parts of dataset

### Applications

-> Principal Component Analysis for Dimensionality Reduciton helps increase quality of data

-> Wokring and applying transfomations on pictures

-> Encoding of the dataset

-> singular value decomposition

-> NLP

-> Optimisation of Deep learning models
